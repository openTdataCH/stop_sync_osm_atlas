## HRDF COMPLETE GUIDE

### 1. What is HRDF?
HAFAS Raw Data Format (HRDF) is a fixed-width, line-oriented timetable dump published annually by the Swiss Open Transport Data platform.  It covers all Swiss public-transport services (and a small border area) for an entire schedule year and encodes:
* Master data — stops, companies, lines, directions, etc.
* Timetable data — every single trip with calendars, attributes, line references, directions, etc.
* Transfer & infrastructure data — platforms (GLEISE), transfer times, meta-stations, etc.

Unlike GTFS (relational CSV) or NeTEx (XML), HRDF is **not** self-describing: »fields« live at fixed byte positions that must be interpreted with the RV spec.

---

### 2. Where do we touch HRDF in this codebase?
```mermaid
graph TB
    subgraph Data acquisition
        A[download_and_extract_hrdf()] -->|ZIP→folder| B(oev_sammlung_ch_hrdf_* )
    end
    subgraph Processing
        B --> C[parse_gleise_lv95_for_sloids]
        B --> D[extract_fplan_directions_for_trips]
        B --> E[load_station_names_hrdf]
        C --> F(process_hrdf_direction_data)
        D --> F
        E --> F
        F -->|CSV| G[atlas_routes_hrdf.csv]
    end
    subgraph Consumption
        G --> H[_perform_hrdf_matching in route_matching.py]
        G --> I[load_route_data() → import_data_db.py]
        H --> J[final_pipeline()] 
        I --> K[RouteAndDirection ETL]
    end
```
(See the Python symbols inside `get_atlas_data.py`, `route_matching.py`, `import_data_db.py`.)

---

### 3. Step-by-step pipeline

The process of extracting route information from HRDF and using it for matching involves several steps, primarily orchestrated by `get_atlas_data.py`.

1.  **Download & Unzip (`download_and_extract_hrdf`)**  
    This function downloads the HRDF data as a ZIP file from the OpenTransportData platform and extracts its contents into the `data/raw/` directory. The function skips the download if the target folder already exists, so the folder must be manually deleted to fetch a newer version.

2.  **Select Target SLOIDs**  
    The pipeline begins by loading the `stops_ATLAS.csv` file (generated by a previous step) into a `traffic_points` DataFrame. It compiles a set of all unique `sloid` values from this data. These are the stops for which we will attempt to find HRDF route information.

3.  **Map SLOIDs to Trips (`parse_gleise_lv95_for_sloids`)**  
    This is the first key parsing step. The function reads `GLEISE_LV95`, a large file that links track infrastructure to trips.
    *   **Goal:** To find which trips (`(trip_no, op_no)`) serve which SLOIDs.
    *   **Method:** The file is parsed in two stages:
        1.  It first identifies lines that define a SLOID and associate it with a `(UIC, ref_no)` pair. These lines are marked with `g A`.
        2.  It then finds lines that associate a `(UIC, ref_no)` pair with one or more trips, identified by `(trip_no, op_no)`.
    *   **Output:** A dictionary `sloid_to_trips` mapping each target SLOID to a list of trip identifiers it serves. A "trip identifier" in this context is a unique tuple of `(trip_number, operator_number)`. For example:
        ```python
        {
            'ch:1:sloid:8503000:13': [
                ('001234', '000011'),  # Trip 1234 from operator 11 (SBB)
                ('005678', '000011')   # Trip 5678 from operator 11 (SBB)
            ],
            # ... other sloids
        }
        ```

    The `GLEISE_LV95` file contains several record types to link logical trips to physical platforms. A single piece of infrastructure (like a platform) is identified by the combination of a station's UIC number and a "Link ID" (e.g., `#0000001`).

    *   **Trip Association Records** have the format `[Station UIC] [Trip No] [Op No] [Link ID] [Bitfield]`. They declare that a specific trip uses a specific piece of infrastructure.
        *   Example: `8500010 000003 000011 #0000001 053751` indicates trip `000003` at station `8500010` uses the infrastructure identified by link `#0000001`.
    *   **Infrastructure Definition Records** describe the properties of that infrastructure:
        *   **SLOID Assignment (`g A`)**: `[Station UIC] [Link ID] g A [SLOID]`. This is the most critical record for our process, as it assigns the official SLOID to the infrastructure. Example: `8574200 #0000003 g A ch:1:sloid:74200:1:3`.
        *   **Platform/Track Name (`G`)**: `[Station UIC] [Link ID] G '[Name]'`. This gives a name (e.g., '11') to the platform.
        *   **Coordinates (`k`)**: `[Station UIC] [Link ID] k [LV95 Easting] [LV95 Northing] [Altitude]`. This provides the geodata.

4.  **Extract Trip Termini from FPLAN (`extract_fplan_directions_for_trips`)**  
    With a list of all relevant trips from the previous step, this function parses the main timetable file, `FPLAN`.
    *   **Goal:** For each trip, find its line name and its start and end stations.
    *   **Method:** The `FPLAN` file is processed sequentially. When a trip header (`*Z`) matching one of our target trips is found, the function records:
        *   The line name from the `*L` record.
        *   The UIC codes of the first and last stops in the trip's stop list.
    *   **Output:** A dictionary `trip_directions` mapping a trip identifier to its line and its start/end stop UICs.

5.  **Load Station Names (`load_station_names_hrdf`)**  
    This is a simple helper function that reads the `BAHNHOF` file to create a mapping from a station's UIC code to its official name.

6.  **Synthesize and Save Direction Data (`process_hrdf_direction_data`)**  
    This function aggregates the results from the previous steps.
    *   **Goal:** Create a clean, usable CSV file containing direction strings for each SLOID.
    *   **Method:** It iterates through the `sloid_to_trips` map. For each trip associated with a SLOID, it looks up the trip's direction information and the names of the start/end stations. It then constructs two types of direction strings:
        1.  `direction_name`: A human-readable string, e.g., "Genève → Zürich HB".
        2.  `direction_uic`: A UIC-based string, e.g., "8501008 → 8503000".
    *   **Output:** A CSV file, `data/processed/atlas_routes_hrdf.csv`, with columns `line_name`, `sloid`, `direction_name`, and `direction_uic`. A single SLOID can have multiple rows if it is served by trips with different destinations.

7.  **Consume Direction Data for Matching**  
    The generated `atlas_routes_hrdf.csv` file is used in two key places:
    *   **During Matching (`_perform_hrdf_matching` in `route_matching.py`):**
        *   The function attempts to match remaining unmatched ATLAS stops to OSM nodes.
        *   It creates direction strings for OSM nodes by parsing route relations in `osm_data.xml`.
        *   A match is created (`route_hrdf_*`) if an ATLAS stop and an OSM node share the **same UIC reference** and have at least one **common direction string** (either name-based or UIC-based).
    *   **During Database Import (`load_route_data` in `import_data_db.py`):**
        *   The HRDF route data is loaded from the CSV and attached to the corresponding `AtlasStop` record in the database as a JSON object. This makes the direction information available to the backend application for display and analysis.

---

### 4. Why are we “missing” route data?

1. **SLOID filter** – only SLOIDs present in `traffic_points` are considered.  If ATLAS lacks a sloid for a platform or the sloid resides in `BHFART_60` only, it is ignored.
2. **GLEISE ↔ FPLAN linkage** – if a given sloid does not appear in **both** parts of `GLEISE_LV95` _and_ its trips aren’t found in `FPLAN` (common for auxiliary or inactive tracks) no direction can be created.
3. **Direction deduplication** – we aggregate directions per sloid but **drop duplicates**; edge-cases where the only unique direction is filtered away will leave the sloid w/o rows.
4. **Unmapped OSM relation** – even with HRDF directions, the route matching still needs an OSM relation whose `type=route` members include the candidate node.  Missing or incomplete relation tagging will prevent matches and will manifest as “stop has HRDF but no OSM route”.
5. **Data vintage mismatch** – HRDF 5.40.41 (2025) vs. most recent OSM snapshot.  Lines may have changed.

---

### 5. Audit checklist

| ✅ | Check | Command / File |
|----|-------|----------------|
|    | HRDF zip downloaded & folder present | `ls oev_sammlung_ch_hrdf_*` |
|    | `atlas_routes_hrdf.csv` not empty | `wc -l data/processed/atlas_routes_hrdf.csv` |
|    | Percentage of ATLAS sloids that got HRDF directions | see printed stats in `get_atlas_data.py` |
|    | `route_hrdf_*` matches appear in final summary | run `matching_process.final_pipeline()` |
|    | For unmatched HRDF sloids, see if FPLAN trips exist | search in `FPLAN` |
|    | For matched HRDF but still unmatched overall, verify OSM relation | Overpass query or JOSM |

---

### 6. Suggestions to improve coverage

1. **Use both `GLEISE_WGS` and `GLEISE_LV95`**  
   The Swiss 2024+ export replaces legacy files; parsing only LV95 may miss WGS-only rows where coordinates or sloids are attached.
2. **Fallback when no `line_name`**  
   Current code discards directions whose `line_name` starts with `REF`.  Consider keeping these – you can still match by direction strings even without a line.
3. **Leverage `RICHTUNG`**  
   The *_direction text_* file provides semantic headings not covered by FPLAN start/​end pair.  Could be added to enrich `direction_name`.
4. **Import auxiliary stops**  
   HRDF has helper IDs (< 1 000 000).  Including them would allow grouping by meta-stations and maybe recover route info via grouping.
5. **Cross-year merge**  
   If a stop is missing this year, look at previous year’s HRDF to back-fill (stops rarely change direction dramatically).
6. **QA dashboard**  
    Export a list of sloids with flags: `{ has_fplan:bool, has_gleise:bool, matched_osm:bool }` for quick inspection.

---

### 7. Key HRDF Files
| File | Purpose |
|------|---------|
| `FPLAN` | The main timetable file, containing all trips with their headers, attributes, and stop lists. It's the largest file in the dataset. |
| `GLEISE_LV95` / `GLEISE_WGS` | Links physical infrastructure (platforms, tracks identified by SLOID) to logical trips. Critical for associating a stop with the routes that serve it. |
| `BAHNHOF` | Contains the mapping from a stop's 7-digit UIC number to its official name. |
| `BHFART` | Defines properties of stops, including their SLOID assignments and usage restrictions (e.g., if a stop can be used as a start/destination). |
| `RICHTUNG` | Provides human-readable direction texts (e.g., "Direction Zurich HB") associated with a direction code. |

---

### 8. Troubleshooting FAQs

**Q : `atlas_routes_hrdf.csv` is empty**  
A : Check that the HRDF folder matches the regex in `download_and_extract_hrdf`; also ensure columns `sloid`, `number`, `designation` exist in `stops_ATLAS.csv`.

**Q : `route_hrdf_*` matching returns 0 rows**  
A : Likely the `_get_osm_directions_from_xml` could not infer direction strings because OSM relations are missing endpoints with `name`/`uic_ref` tags.  Inspect relation membership.

**Q : Many stops have HRDF directions but still no OSM match**  
A : Verify OSM snapshot date; line relations might have been added after the dump.  Consider updating `osm_data.xml`.

---

### 9. Next actions
1. Extend parser to read **GLEISE_WGS** and/or the new 2025 structure (g A / k lines).  
2. Re-run `get_atlas_data.py` with `DOWNLOAD_AND_PROCESS_GTFS=False` to focus on HRDF and regenerate `atlas_routes_hrdf.csv`.
3. Add a pytest that fails if HRDF coverage < xx % of ATLAS sloids.
4. Periodically refresh OSM snapshot and compare `route_hrdf` match counts.

_This document was generated on $(date) and should be kept in sync with code updates._ 